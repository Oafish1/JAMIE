{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f57ea00-b9ec-4cfe-a819-4e16d365d6b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c92eb915-cb7c-4663-a778-c7d28524c587",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-04 06:22:48.037038: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-04 06:22:48.596477: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-04 06:22:48.679366: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-01-04 06:22:48.679399: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-01-04 06:22:48.731245: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-01-04 06:22:50.452938: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-04 06:22:50.453167: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-04 06:22:50.453178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/thema/miniconda3/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from jamie import JAMIE\n",
    "from jamie.evaluation import *\n",
    "from jamie.utilities import *\n",
    "import matplotlib.pyplot as plt\n",
    "from mmd_wrapper import mmd_combine\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8901cbfb-73c9-4099-8865-9f4efa8f1262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0009ecf8-9bba-4ef8-9900-fefdda586267",
   "metadata": {},
   "source": [
    "# MMD-MA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6206e91-f09d-4d3b-81b1-461ea9c57df2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14435/4179390009.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  type1 = type1.astype(np.int)\n",
      "/tmp/ipykernel_14435/4179390009.py:10: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  type2 = type2.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'MMD-MA'\n",
    "dataset_color = 'lime'\n",
    "modality_names = ['Modality 1', 'Modality 2']\n",
    "data_folder = '../data/UnionCom/MMD/'\n",
    "data1 = np.loadtxt(data_folder + \"s1_mapped1.txt\")\n",
    "data2 = np.loadtxt(data_folder + \"s1_mapped2.txt\")\n",
    "type1 = np.loadtxt(data_folder + \"s1_type1.txt\")\n",
    "type2 = np.loadtxt(data_folder + \"s1_type2.txt\")\n",
    "type1 = type1.astype(np.int)\n",
    "type2 = type2.astype(np.int)\n",
    "type1 = np.array([f'Cell Type {i}' for i in type1])\n",
    "type2 = np.array([f'Cell Type {i}' for i in type2])\n",
    "\n",
    "# Labels\n",
    "labels = [type1, type2]\n",
    "features = [None, None]\n",
    "feature_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd13b4ae-ce65-4838-a18a-29f61bc9d0fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "data1 = preprocessing.scale(data1, axis=0)\n",
    "data2 = preprocessing.scale(data2, axis=0)\n",
    "data1[np.isnan(data1)] = 0  # Replace NaN with average\n",
    "data2[np.isnan(data2)] = 0\n",
    "# data1 = preprocessing.MinMaxScaler().fit_transform(data1)\n",
    "# data2 = preprocessing.MinMaxScaler().fit_transform(data2)\n",
    "dataset = [data1, data2]\n",
    "\n",
    "# Replace NULL feature names\n",
    "for i in range(len(features)):\n",
    "    if features[i] is None:\n",
    "        features[i] = np.array([f'Feature {i}' for i in range(dataset[i].shape[1])])\n",
    "        \n",
    "# Train-Test Imputation\n",
    "train_size = int(.8 * len(data1))\n",
    "train_idx = np.random.choice(range(len(data1)), train_size, replace=False)\n",
    "test_idx = np.array(list(set(range(len(data1))) - set(train_idx)))\n",
    "\n",
    "# Reduced Priors\n",
    "full_priors = np.eye(len(dataset[0]))\n",
    "random_idx = np.random.choice(range(len(dataset[0])), int(.5 * len(dataset[0])), replace=False)\n",
    "priors = np.zeros(len(dataset[0]))\n",
    "priors[random_idx] = 1\n",
    "half_priors = np.diag(priors)\n",
    "random_idx = np.random.choice(range(len(dataset[0])), int(.75 * len(dataset[0])), replace=False)\n",
    "priors = np.zeros(len(dataset[0]))\n",
    "priors[random_idx] = 1\n",
    "tq_priors = np.diag(priors)\n",
    "none_priors = np.zeros((len(dataset[0]), len(dataset[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6b6aa83-78c0-4450-a8aa-19676e9567d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use random seed: 666\n",
      "Shape of Raw data\n",
      "Dataset 0: (300, 2000)\n",
      "Dataset 1: (300, 1000)\n",
      "Device: cpu\n",
      "---------------------------------\n",
      "Find correspondence between Dataset 1 and Dataset 2\n",
      "epoch:[500/2000] err:11.7152 alpha:0.1578\n",
      "epoch:[1000/2000] err:10.2456 alpha:0.3746\n",
      "epoch:[1500/2000] err:10.1076 alpha:0.4565\n",
      "epoch:[2000/2000] err:10.0670 alpha:0.4759\n",
      "Finished Matching!\n",
      "---------------------------------\n",
      "Train coupled autoencoders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/nck/repos/nmacom/jamie/jamie.py:427: UserWarning: PCA dim must be lower than 300, found 512, adjusting to compensate.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/nck/repos/nmacom/jamie/jamie.py:427: UserWarning: PCA dim must be lower than 300, found 512, adjusting to compensate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL: 0.0077  Rec: 0.3936  CosSim: 0.1246  F: 0.0076\n",
      "Finished Mapping!\n",
      "---------------------------------\n",
      "JAMIE Done!\n",
      "Distance: 0.863750461001473\n",
      "Distance Memory: Stored 1777383 - Peak 2772492\n",
      "Correspondence: 25.195908704998146\n",
      "Correspondence Memory: Stored 372504 - Peak 1441931\n",
      "Mapping: 5.137437727003999\n",
      "Mapping Memory: Stored 8881502 - Peak 18051533\n",
      "Total: 31.197096893003618\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jm = JAMIE(epoch_DNN=100, debug=True, enable_memory_logging=True)\n",
    "jm_data = jm.fit_transform(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e06e561-3e59-4e8d-8193-70679f01ded9",
   "metadata": {},
   "source": [
    "# Simulation 1250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ad8448-6b2e-4305-945c-fda8da7297ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = 'scMultiSim-1250'\n",
    "dataset_color = 'teal'\n",
    "modality_names = ['Modality 1', 'Modality 2']\n",
    "data_folder = '../data/scMultiSim/new/'\n",
    "data1 = np.loadtxt(data_folder + \"scMultiSim_RNA_counts_1250_genes.csv\", delimiter=\",\", skiprows=1)\n",
    "data2 = np.loadtxt(data_folder + \"scMultiSim_ATAC_seq_1250_genes_new.csv\", delimiter=\",\", skiprows=1)\n",
    "ct = pd.read_csv(data_folder + \"cell_meta_1250_genes.csv\").iloc[3, 1:].to_numpy().astype(int)\n",
    "type1 = type2 = np.array([f'Cell Type {i}' for i in ct])\n",
    "\n",
    "# Labels\n",
    "labels = [type1, type2]\n",
    "features = [None, None]\n",
    "feature_dict = {}\n",
    "\n",
    "# Utility\n",
    "positivize = lambda X: [x + x.min() for x in X]\n",
    "minmax = lambda X: [(x + x.min()) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd74c79d-fece-45fb-a79c-a3409bac767c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "data1 = preprocessing.scale(data1, axis=0)\n",
    "data2 = preprocessing.scale(data2, axis=0)\n",
    "data1[np.isnan(data1)] = 0  # Replace NaN with average\n",
    "data2[np.isnan(data2)] = 0\n",
    "# data1 = preprocessing.MinMaxScaler().fit_transform(data1)\n",
    "# data2 = preprocessing.MinMaxScaler().fit_transform(data2)\n",
    "dataset = [data1, data2]\n",
    "\n",
    "# Replace NULL feature names\n",
    "for i in range(len(features)):\n",
    "    if features[i] is None:\n",
    "        features[i] = np.array([f'Feature {i}' for i in range(dataset[i].shape[1])])\n",
    "        \n",
    "# Train-Test Imputation\n",
    "train_size = int(.8 * len(data1))\n",
    "train_idx = np.random.choice(range(len(data1)), train_size, replace=False)\n",
    "test_idx = np.array(list(set(range(len(data1))) - set(train_idx)))\n",
    "\n",
    "# Reduced Priors\n",
    "full_priors = np.eye(len(dataset[0]))\n",
    "random_idx = np.random.choice(range(len(dataset[0])), int(.5 * len(dataset[0])), replace=False)\n",
    "priors = np.zeros(len(dataset[0]))\n",
    "priors[random_idx] = 1\n",
    "half_priors = np.diag(priors)\n",
    "random_idx = np.random.choice(range(len(dataset[0])), int(.75 * len(dataset[0])), replace=False)\n",
    "priors = np.zeros(len(dataset[0]))\n",
    "priors[random_idx] = 1\n",
    "tq_priors = np.diag(priors)\n",
    "none_priors = np.zeros((len(dataset[0]), len(dataset[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81100a48-7ed9-4db9-a4b3-c36a48291dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use random seed: 666\n",
      "Shape of Raw data\n",
      "Dataset 0: (500, 1250)\n",
      "Dataset 1: (500, 3750)\n",
      "Device: cpu\n",
      "---------------------------------\n",
      "Find correspondence between Dataset 1 and Dataset 2\n",
      "epoch:[500/2000] err:13.0043 alpha:0.0774\n",
      "epoch:[1000/2000] err:16.3444 alpha:0.1053\n",
      "epoch:[1500/2000] err:18.7388 alpha:0.1709\n",
      "epoch:[2000/2000] err:21.0007 alpha:0.4030\n",
      "Finished Matching!\n",
      "---------------------------------\n",
      "Train coupled autoencoders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/nck/repos/nmacom/jamie/jamie.py:427: UserWarning: PCA dim must be lower than 500, found 512, adjusting to compensate.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/nck/repos/nmacom/jamie/jamie.py:427: UserWarning: PCA dim must be lower than 500, found 512, adjusting to compensate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL: 0.0116  Rec: 1.1089  CosSim: 0.0667  F: 0.0054\n",
      "Finished Mapping!\n",
      "---------------------------------\n",
      "JAMIE Done!\n",
      "Distance: 0.3170076840033289\n",
      "Distance Memory: Stored 4164346 - Peak 6543329\n",
      "Correspondence: 16.31046741300088\n",
      "Correspondence Memory: Stored 1015342 - Peak 4001195\n",
      "Mapping: 15.335240792999684\n",
      "Mapping Memory: Stored 24233878 - Peak 62105969\n",
      "Total: 31.962715890003892\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jm = JAMIE(epoch_DNN=100, debug=True, enable_memory_logging=True)\n",
    "jm_data = jm.fit_transform(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbedecc8-d72a-4788-a391-ccac6d27eabc",
   "metadata": {},
   "source": [
    "# scMNC Motor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "649c0247-23a2-4cd7-9eed-d646e336bc1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = 'scMNC-Motor'\n",
    "dataset_color = 'sienna'\n",
    "modality_names = ['Gene Expression', 'Electrophysiology']\n",
    "data_folder = '../data/scMNC/mouse_motor_cortex/data/'\n",
    "data1 = pd.read_csv(data_folder + \"geneExp_filtered.csv\")\n",
    "data2 = pd.read_csv(data_folder + \"efeature_filtered.csv\")\n",
    "feat1 = np.array(data1.iloc[:, 0])\n",
    "feat2 = np.array(data2.columns)\n",
    "sample_names1 = data1.columns[1:]\n",
    "assert ((data1.shape[1] - 1) == data2.shape[0])\n",
    "data1 = np.transpose(np.array(data1)[:, 1:])\n",
    "data2 = np.array(data2)\n",
    "meta = pd.read_excel(data_folder + \"motor_meta_data.xlsx\")[['Cell', 'RNA family']]\n",
    "meta = np.array(meta)\n",
    "meta_idx = [np.argwhere(meta[:, 0] == sample_names1[i])[0][0] for i in range(sample_names1.shape[0])]\n",
    "type1 = type2 = np.array([x.split()[0] for x in meta[meta_idx, 1]])\n",
    "\n",
    "# Sampling\n",
    "# split = 1000 # data1.shape[0]\n",
    "# data_col_idx = np.random.choice(range(data1.shape[0]), split, replace=False)\n",
    "# data1, data2, type1, type2 = (x[data_col_idx] for x in (data1, data2, type1, type2))\n",
    "\n",
    "# Labels\n",
    "labels = [type1, type2]\n",
    "features = [feat1, feat2]\n",
    "feature_dict = {}\n",
    "\n",
    "# Utility\n",
    "positivize = lambda X: [x + x.min() for x in X]\n",
    "minmax = lambda X: [(x + x.min()) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "466c6d49-89ed-459d-aeff-86d4e929a6e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "data1 = preprocessing.scale(data1, axis=0)\n",
    "data2 = preprocessing.scale(data2, axis=0)\n",
    "data1[np.isnan(data1)] = 0  # Replace NaN with average\n",
    "data2[np.isnan(data2)] = 0\n",
    "# data1 = preprocessing.MinMaxScaler().fit_transform(data1)\n",
    "# data2 = preprocessing.MinMaxScaler().fit_transform(data2)\n",
    "dataset = [data1, data2]\n",
    "\n",
    "# Replace NULL feature names\n",
    "for i in range(len(features)):\n",
    "    if features[i] is None:\n",
    "        features[i] = np.array([f'Feature {i}' for i in range(dataset[i].shape[1])])\n",
    "        \n",
    "# Train-Test Imputation\n",
    "train_size = int(.8 * len(data1))\n",
    "train_idx = np.random.choice(range(len(data1)), train_size, replace=False)\n",
    "test_idx = np.array(list(set(range(len(data1))) - set(train_idx)))\n",
    "\n",
    "# Reduced Priors\n",
    "full_priors = np.eye(len(dataset[0]))\n",
    "random_idx = np.random.choice(range(len(dataset[0])), int(.5 * len(dataset[0])), replace=False)\n",
    "priors = np.zeros(len(dataset[0]))\n",
    "priors[random_idx] = 1\n",
    "half_priors = np.diag(priors)\n",
    "random_idx = np.random.choice(range(len(dataset[0])), int(.75 * len(dataset[0])), replace=False)\n",
    "priors = np.zeros(len(dataset[0]))\n",
    "priors[random_idx] = 1\n",
    "tq_priors = np.diag(priors)\n",
    "none_priors = np.zeros((len(dataset[0]), len(dataset[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c06c1ad-7f39-4902-906b-ce4a494f506a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use random seed: 666\n",
      "Shape of Raw data\n",
      "Dataset 0: (1208, 1286)\n",
      "Dataset 1: (1208, 29)\n",
      "Device: cpu\n",
      "---------------------------------\n",
      "Find correspondence between Dataset 1 and Dataset 2\n",
      "epoch:[500/2000] err:0.3861 alpha:0.0050\n",
      "epoch:[1000/2000] err:3.3359 alpha:0.0249\n",
      "epoch:[1500/2000] err:5.7469 alpha:0.0598\n",
      "epoch:[2000/2000] err:7.0837 alpha:0.0926\n",
      "Finished Matching!\n",
      "---------------------------------\n",
      "Train coupled autoencoders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/nck/repos/nmacom/jamie/jamie.py:427: UserWarning: PCA dim must be lower than 29, found 512, adjusting to compensate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL: 0.0033  Rec: 1.7153  CosSim: 1.3200  F: 0.0222\n",
      "Finished Mapping!\n",
      "---------------------------------\n",
      "JAMIE Done!\n",
      "Distance: 1.8937601689976873\n",
      "Distance Memory: Stored 23527186 - Peak 36739750\n",
      "Correspondence: 172.87693743099953\n",
      "Correspondence Memory: Stored 5866579 - Peak 23349419\n",
      "Mapping: 11.145951522004907\n",
      "Mapping Memory: Stored 10897745 - Peak 43899487\n",
      "Total: 185.91664912200213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jm = JAMIE(epoch_DNN=100, debug=True, enable_memory_logging=True)\n",
    "jm_data = jm.fit_transform(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3f5571-540c-44d4-942a-78d343134476",
   "metadata": {},
   "source": [
    "# scMNC Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95000a46-6f3d-4be3-9c8a-44a70c0b6897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = 'scMNC-Visual'\n",
    "dataset_color = 'magenta'\n",
    "modality_names = ['Gene Expression', 'Electrophysiology']\n",
    "data_folder = '../data/scMNC/mouse_visual_cortex/data/'\n",
    "data1 = pd.read_csv(data_folder + \"geneExp_filtered.csv\")\n",
    "data2 = pd.read_csv(data_folder + \"efeature_filtered.csv\")\n",
    "sample_names1 = data1.columns[1:]\n",
    "sample_names2 = np.array(data2)[:, 0]\n",
    "feature_names1 = data1.iloc[:,0]\n",
    "feature_names2 = data2.columns[3:]\n",
    "assert (sample_names1 == sample_names2).all()\n",
    "data1 = np.transpose(np.array(data1)[:, 1:])\n",
    "data2 = np.array(data2)[:, 3:]\n",
    "meta = pd.read_csv(data_folder + \"20200711_patchseq_metadata_mouse.csv\")[['transcriptomics_sample_id', 't_type']]\n",
    "meta = np.array(meta)\n",
    "meta_idx = [np.argwhere(meta[:, 0] == sample_names1[i])[0][0] for i in range(sample_names1.shape[0])]\n",
    "type1 = type2 = np.array([x.split(' ')[0] for x in meta[meta_idx, 1]])\n",
    "\n",
    "# Sampling\n",
    "# split = 1000 # data1.shape[0]\n",
    "# data_col_idx = np.random.choice(range(data1.shape[0]), split, replace=False)\n",
    "# data1, data2, type1, type2 = (x[data_col_idx] for x in (data1, data2, type1, type2))\n",
    "\n",
    "# Labels\n",
    "labels = [type1, type2]\n",
    "features = [np.array(feature_names1), np.array(feature_names2)]\n",
    "feature_dict = {}\n",
    "\n",
    "# Utility\n",
    "positivize = lambda X: [x + x.min() for x in X]\n",
    "minmax = lambda X: [(x + x.min()) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "790ad528-85db-40f9-82d9-585e792985cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "data1 = preprocessing.scale(data1, axis=0)\n",
    "data2 = preprocessing.scale(data2, axis=0)\n",
    "data1[np.isnan(data1)] = 0  # Replace NaN with average\n",
    "data2[np.isnan(data2)] = 0\n",
    "# data1 = preprocessing.MinMaxScaler().fit_transform(data1)\n",
    "# data2 = preprocessing.MinMaxScaler().fit_transform(data2)\n",
    "dataset = [data1, data2]\n",
    "\n",
    "# Replace NULL feature names\n",
    "for i in range(len(features)):\n",
    "    if features[i] is None:\n",
    "        features[i] = np.array([f'Feature {i}' for i in range(dataset[i].shape[1])])\n",
    "        \n",
    "# Train-Test Imputation\n",
    "train_size = int(.8 * len(data1))\n",
    "train_idx = np.random.choice(range(len(data1)), train_size, replace=False)\n",
    "test_idx = np.array(list(set(range(len(data1))) - set(train_idx)))\n",
    "\n",
    "# Reduced Priors\n",
    "full_priors = np.eye(len(dataset[0]))\n",
    "random_idx = np.random.choice(range(len(dataset[0])), int(.5 * len(dataset[0])), replace=False)\n",
    "priors = np.zeros(len(dataset[0]))\n",
    "priors[random_idx] = 1\n",
    "half_priors = np.diag(priors)\n",
    "random_idx = np.random.choice(range(len(dataset[0])), int(.75 * len(dataset[0])), replace=False)\n",
    "priors = np.zeros(len(dataset[0]))\n",
    "priors[random_idx] = 1\n",
    "tq_priors = np.diag(priors)\n",
    "none_priors = np.zeros((len(dataset[0]), len(dataset[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bde8e0f1-ef29-490c-b06e-e4c1da15bf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use random seed: 666\n",
      "Shape of Raw data\n",
      "Dataset 0: (3654, 1302)\n",
      "Dataset 1: (3654, 39)\n",
      "Device: cpu\n",
      "---------------------------------\n",
      "Find correspondence between Dataset 1 and Dataset 2\n",
      "epoch:[500/2000] err:0.4526 alpha:0.0207\n",
      "epoch:[1000/2000] err:0.0000 alpha:0.0000\n",
      "epoch:[1500/2000] err:0.0000 alpha:0.0000\n",
      "epoch:[2000/2000] err:0.0000 alpha:0.0000\n",
      "Finished Matching!\n",
      "---------------------------------\n",
      "Train coupled autoencoders\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/nck/repos/nmacom/jamie/jamie.py:427: UserWarning: PCA dim must be lower than 39, found 512, adjusting to compensate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL: 0.0126  Rec: 1.3997  CosSim: 0.0915  F: 0.0023\n",
      "Finished Mapping!\n",
      "---------------------------------\n",
      "JAMIE Done!\n",
      "Distance: 39.65111083200463\n",
      "Distance Memory: Stored 213777111 - Peak 334178279\n",
      "Correspondence: 4496.191194003004\n",
      "Correspondence Memory: Stored 53474886 - Peak 213628651\n",
      "Mapping: 49.011749792000046\n",
      "Mapping Memory: Stored 22045795 - Peak 235707328\n",
      "Total: 4584.854054627009\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jm = JAMIE(epoch_DNN=100, debug=True, enable_memory_logging=True)\n",
    "jm_data = jm.fit_transform(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacab1a8-b775-423e-9a5f-7a930c70d473",
   "metadata": {},
   "source": [
    "# DM_rep4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f673b8f-7815-494f-bc7a-1b1e6d698502",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thema/miniconda3/lib/python3.9/site-packages/anndata/compat/__init__.py:232: FutureWarning: Moving element from .uns['neighbors']['distances'] to .obsp['distances'].\n",
      "\n",
      "This is where adjacency matrices should go now.\n",
      "  warn(\n",
      "/home/thema/miniconda3/lib/python3.9/site-packages/anndata/compat/__init__.py:232: FutureWarning: Moving element from .uns['neighbors']['connectivities'] to .obsp['connectivities'].\n",
      "\n",
      "This is where adjacency matrices should go now.\n",
      "  warn(\n",
      "/home/thema/miniconda3/lib/python3.9/site-packages/anndata/compat/__init__.py:232: FutureWarning: Moving element from .uns['neighbors']['distances'] to .obsp['distances'].\n",
      "\n",
      "This is where adjacency matrices should go now.\n",
      "  warn(\n",
      "/home/thema/miniconda3/lib/python3.9/site-packages/anndata/compat/__init__.py:232: FutureWarning: Moving element from .uns['neighbors']['connectivities'] to .obsp['connectivities'].\n",
      "\n",
      "This is where adjacency matrices should go now.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "babel_dir = '../data/babel_data/DM_rep4/'\n",
    "train = sc.read_h5ad(babel_dir + 'train_rna.h5ad')\n",
    "v = train.var_names\n",
    "train = train.X.toarray()\n",
    "valid = sc.read_h5ad(babel_dir + 'truth_rna.h5ad').X.toarray()\n",
    "data1 = np.concatenate([train, valid], axis=0)\n",
    "fnames1 = np.array(v)\n",
    "\n",
    "train = sc.read_h5ad(babel_dir + 'train_atac.h5ad')\n",
    "v = train.var_names\n",
    "train = train.X.toarray()\n",
    "valid = sc.read_h5ad(babel_dir + 'truth_atac.h5ad').X.toarray()\n",
    "data2 = np.concatenate([train, valid], axis=0)\n",
    "fnames2 = np.array(v)\n",
    "\n",
    "split = train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dda7b4c1-7472-437f-a23f-b849f2fe9623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = 'DM_rep4'\n",
    "dataset_color = 'black'\n",
    "modality_names = ['RNA', 'ATAC']\n",
    "\n",
    "# data1 = PCA(n_components=32).fit_transform(data1)\n",
    "# data2 = PCA(n_components=32).fit_transform(data2)\n",
    "# fnames1 = fnames2 = None\n",
    "\n",
    "type1 = np.array(len(data1) * ['Cell Type 0'])\n",
    "type2 = np.array(len(data2) * ['Cell Type 0'])\n",
    "\n",
    "# Sampling\n",
    "# sample_num = 500\n",
    "# data_col_idx = np.random.choice(range(split), sample_num, replace=False)\n",
    "# data1, data2, type1, type2 = (x[list(data_col_idx) + list(range(split, len(data1)))] for x in (data1, data2, type1, type2))\n",
    "# split = sample_num\n",
    "\n",
    "# Labels\n",
    "labels = [type1, type2]\n",
    "features = [np.array(fnames1), np.array(fnames2)]\n",
    "feature_dict = {}\n",
    "\n",
    "# Utility\n",
    "positivize = lambda X: [x + x.min() for x in X]\n",
    "minmax = lambda X: [(x + x.min()) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5733050e-4c08-475b-b5d5-497e2220e2fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thema/miniconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:235: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\n",
      "/home/thema/miniconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:254: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\n",
      "/home/thema/miniconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:235: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\n",
      "/home/thema/miniconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:254: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "data1 = preprocessing.scale(data1, axis=0)\n",
    "data2 = preprocessing.scale(data2, axis=0)\n",
    "data1[np.isnan(data1)] = 0  # Replace NaN with average\n",
    "data2[np.isnan(data2)] = 0\n",
    "# data1 = preprocessing.MinMaxScaler().fit_transform(data1)\n",
    "# data2 = preprocessing.MinMaxScaler().fit_transform(data2)\n",
    "dataset = [data1, data2]\n",
    "\n",
    "# Replace NULL feature names\n",
    "for i in range(len(features)):\n",
    "    if features[i] is None:\n",
    "        features[i] = np.array([f'Feature {i}' for i in range(dataset[i].shape[1])])\n",
    "        \n",
    "# # Train-Test Imputation\n",
    "train_size = split\n",
    "train_idx = np.array(range(split))\n",
    "test_idx = np.array(list(set(range(len(data1))) - set(train_idx)))\n",
    "\n",
    "# Reduced Priors\n",
    "full_priors = np.eye(len(dataset[0]))\n",
    "random_idx = np.random.choice(range(len(dataset[0])), int(.5 * len(dataset[0])), replace=False)\n",
    "priors = np.zeros(len(dataset[0]))\n",
    "priors[random_idx] = 1\n",
    "half_priors = np.diag(priors)\n",
    "none_priors = np.zeros((len(dataset[0]), len(dataset[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2afce45f-579a-4e29-8e5e-3a93b07c1515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use random seed: 666\n",
      "Shape of Raw data\n",
      "Dataset 0: (4301, 34861)\n",
      "Dataset 1: (4301, 85596)\n",
      "Device: cpu\n",
      "---------------------------------\n",
      "Find correspondence between Dataset 1 and Dataset 2\n",
      "epoch:[500/2000] err:3240.7747 alpha:318.6570\n",
      "epoch:[1000/2000] err:202.1322 alpha:18.4804\n",
      "epoch:[1500/2000] err:0.6152 alpha:0.0076\n",
      "epoch:[2000/2000] err:0.0001 alpha:0.0000\n",
      "Finished Matching!\n",
      "---------------------------------\n",
      "Train coupled autoencoders\n",
      "KL: 0.0116  Rec: 1.8302  CosSim: 0.0442  F: 0.0019\n",
      "Finished Mapping!\n",
      "---------------------------------\n",
      "JAMIE Done!\n",
      "Distance: 435.5420048619999\n",
      "Distance Memory: Stored 296179081 - Peak 5410347212\n",
      "Correspondence: 7009.672455397995\n",
      "Correspondence Memory: Stored 74064707 - Peak 295978811\n",
      "Mapping: 190.9282204590054\n",
      "Mapping Memory: Stored 287755346 - Peak 3215879529\n",
      "Total: 7636.142680719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jm = JAMIE(epoch_DNN=100, debug=True, enable_memory_logging=True)\n",
    "jm_data = jm.fit_transform(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3baf07c-6d3b-4a7e-98de-65cd7b603779",
   "metadata": {},
   "source": [
    "# brainchromatin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57576f1c-063f-4664-a954-ed998aa04a6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = 'BrainChromatin'\n",
    "dataset_color = 'orange'\n",
    "modality_names = ['RNA', 'ATAC']\n",
    "data_folder = '../data/brainchromatin/'\n",
    "data1 = pd.read_csv(data_folder + \"multiome_rna_counts.tsv\", delimiter='\\t').transpose()\n",
    "data2 = pd.read_csv(data_folder + \"multiome_atac_gene_activities.tsv\", delimiter='\\t', nrows=35000).transpose()\n",
    "data2 = data2.transpose()[data1.index].transpose()\n",
    "\n",
    "meta = pd.read_csv(data_folder + \"multiome_cell_metadata.txt\", delimiter='\\t')\n",
    "meta_names = pd.read_csv(data_folder + \"multiome_cluster_names.txt\", delimiter='\\t')\n",
    "meta_names = meta_names[meta_names['Assay'] == 'Multiome ATAC']\n",
    "meta = pd.merge(meta, meta_names, left_on='ATAC_cluster', right_on='Cluster.ID', how='left')\n",
    "meta.index = meta['Cell.ID']\n",
    "\n",
    "type1 = type2 = np.array(meta.transpose()[data1.index].transpose()['Cluster.Name'])\n",
    "fname1, fname2 = data1.columns, data2.columns\n",
    "data1 = data1.to_numpy()\n",
    "data2 = data2.to_numpy()\n",
    "\n",
    "# Sampling\n",
    "split = 4000 # data1.shape[0]\n",
    "data_row_idx = np.random.choice(range(data1.shape[0]), split, replace=False)\n",
    "# data1, data2, type1, type2 = (x[data_row_idx] for x in (data1, data2, type1, type2))\n",
    "# split_feat_1 = 2000 # data1.shape[1]\n",
    "# data_col1_idx = np.random.choice(range(data1.shape[1]), split_feat_1, replace=False)\n",
    "# data1, fname1 = data1[:, data_col1_idx], fname1[data_col1_idx]\n",
    "# split_feat_2 = 2000 # data2.shape[1]\n",
    "# data_col2_idx = np.random.choice(range(data2.shape[1]), split_feat_2, replace=False)\n",
    "# data2, fname2 = data2[:, data_col2_idx], fname2[data_col2_idx]\n",
    "\n",
    "# Labels\n",
    "labels = [type1, type2]\n",
    "features = [np.array(fname1), np.array(fname2)]\n",
    "feature_dict = {'ENSG00000251562': 'MALAT1', 'ENSG00000153707': 'PTPRD'}\n",
    "\n",
    "# Utility\n",
    "positivize = lambda X: [x + x.min() for x in X]\n",
    "minmax = lambda X: [(x + x.min()) for x in X]\n",
    "\n",
    "# Change Labels\n",
    "group = ['GluN3', 'GluN4', 'IN1', 'GluN2', 'IN2', 'GluN6', 'GluN5', 'RG',\n",
    "       'nIPC', 'GluN1', 'mGPC/OPC', 'IN3', 'IN4', 'SP', 'GluN7',\n",
    "       'MG/EC/Peric.']\n",
    "conv =  ['GluN', 'GluN', 'IN', 'GluN', 'IN', 'GluN', 'GluN', 'RG',\n",
    "        'nIPC', 'GluN', 'mGPC/OPC', 'IN', 'IN', 'SP', 'GluN',\n",
    "        'MG/EC/Peric.']\n",
    "group_conv = {g:c for g, c in zip(group, conv)}\n",
    "labels = [np.array([group_conv[l] for l in label]) for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c8229c8-e313-44f6-986c-6c8743fceb3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "data1 = preprocessing.scale(data1, axis=0)\n",
    "data2 = preprocessing.scale(data2, axis=0)\n",
    "data1[np.isnan(data1)] = 0  # Replace NaN with average\n",
    "data2[np.isnan(data2)] = 0\n",
    "# data1 = preprocessing.MinMaxScaler().fit_transform(data1)\n",
    "# data2 = preprocessing.MinMaxScaler().fit_transform(data2)\n",
    "dataset = [data1, data2]\n",
    "\n",
    "# Replace NULL feature names\n",
    "for i in range(len(features)):\n",
    "    if features[i] is None:\n",
    "        features[i] = np.array([f'Feature {i}' for i in range(dataset[i].shape[1])])\n",
    "        \n",
    "# Train-Test Imputation\n",
    "train_size = int(.8 * len(data1))\n",
    "train_idx = np.random.choice(range(len(data1)), train_size, replace=False)\n",
    "test_idx = np.array(list(set(range(len(data1))) - set(train_idx)))\n",
    "\n",
    "# Reduced Priors\n",
    "full_priors = np.eye(len(dataset[0]))\n",
    "random_idx = np.random.choice(range(len(dataset[0])), int(.5 * len(dataset[0])), replace=False)\n",
    "priors = np.zeros(len(dataset[0]))\n",
    "priors[random_idx] = 1\n",
    "half_priors = np.diag(priors)\n",
    "random_idx = np.random.choice(range(len(dataset[0])), int(.75 * len(dataset[0])), replace=False)\n",
    "priors = np.zeros(len(dataset[0]))\n",
    "priors[random_idx] = 1\n",
    "tq_priors = np.diag(priors)\n",
    "none_priors = np.zeros((len(dataset[0]), len(dataset[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d732e49-2d1a-4050-a6f7-22f0bbebf1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use random seed: 666\n",
      "Shape of Raw data\n",
      "Dataset 0: (8981, 34104)\n",
      "Dataset 1: (8981, 19836)\n",
      "Device: cpu\n",
      "---------------------------------\n",
      "Find correspondence between Dataset 1 and Dataset 2\n",
      "epoch:[500/2000] err:1774.8389 alpha:23.8913\n",
      "epoch:[1000/2000] err:324.3965 alpha:0.0213\n",
      "epoch:[1500/2000] err:152.1128 alpha:0.0015\n",
      "epoch:[2000/2000] err:67.9232 alpha:0.0002\n",
      "Finished Matching!\n",
      "---------------------------------\n",
      "Train coupled autoencoders\n",
      "KL: 0.0116  Rec: 1.7395  CosSim: 0.0402  F: 0.0018\n",
      "Finished Mapping!\n",
      "---------------------------------\n",
      "JAMIE Done!\n",
      "Distance: 694.3321282499965\n",
      "Distance Memory: Stored 1290699169 - Peak 2017201098\n",
      "Correspondence: 44623.54953084101\n",
      "Correspondence Memory: Stored 322791094 - Peak 1290538381\n",
      "Mapping: 270.97620322300645\n",
      "Mapping Memory: Stored 300885686 - Peak 5081428501\n",
      "Total: 45588.85786231401\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jm = JAMIE(epoch_DNN=100, debug=True, enable_memory_logging=True)\n",
    "jm_data = jm.fit_transform(dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b262373c-771f-4b78-bd49-6165bc2ceb2e",
   "metadata": {},
   "source": [
    "# scGLUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6636863-1a27-4570-9aa5-c8418b4e71e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "\n",
    "dataset_name = 'scGLUE'\n",
    "dataset_color = 'yellow'\n",
    "modality_names = ['RNA', 'ATAC']\n",
    "data_folder = '../data/scGLUE/'\n",
    "\n",
    "data1 = sc.read_h5ad(data_folder + 'Chen-2019-RNA.h5ad')\n",
    "type1 = data1.obs.cell_type.to_numpy()\n",
    "fname1 = data1.var.name.to_numpy()\n",
    "data1 = data1.X.todense()\n",
    "\n",
    "data2 = sc.read_h5ad(data_folder + 'Chen-2019-ATAC.h5ad')\n",
    "type2 = data2.obs.cell_type.to_numpy()\n",
    "fname2 = data2.var.index.to_numpy()\n",
    "data2 = data2.X.todense()\n",
    "\n",
    "# Labels\n",
    "labels = [type1, type2]\n",
    "features = [fname1, fname2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cf9e1c5-743c-4f07-999e-c18120ad6de8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thema/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:585: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/thema/miniconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:235: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\n",
      "/home/thema/miniconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:254: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\n",
      "/home/thema/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:585: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/home/thema/miniconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:235: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\n",
      "/home/thema/miniconda3/lib/python3.9/site-packages/sklearn/preprocessing/_data.py:254: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "data1 = preprocessing.scale(data1, axis=0)\n",
    "data2 = preprocessing.scale(data2, axis=0)\n",
    "data1[np.isnan(data1)] = 0  # Replace NaN with average\n",
    "data2[np.isnan(data2)] = 0\n",
    "# data1 = preprocessing.MinMaxScaler().fit_transform(data1)\n",
    "# data2 = preprocessing.MinMaxScaler().fit_transform(data2)\n",
    "dataset = [data1, data2]\n",
    "\n",
    "# Replace NULL feature names\n",
    "for i in range(len(features)):\n",
    "    if features[i] is None:\n",
    "        features[i] = np.array([f'Feature {i}' for i in range(dataset[i].shape[1])])\n",
    "        \n",
    "# Train-Test Imputation\n",
    "train_size = int(.8 * len(data1))\n",
    "train_idx = np.random.choice(range(len(data1)), train_size, replace=False)\n",
    "test_idx = np.array(list(set(range(len(data1))) - set(train_idx)))\n",
    "\n",
    "# Reduced Priors\n",
    "full_priors = np.eye(len(dataset[0]))\n",
    "random_idx = np.random.choice(range(len(dataset[0])), int(.5 * len(dataset[0])), replace=False)\n",
    "priors = np.zeros(len(dataset[0]))\n",
    "priors[random_idx] = 1\n",
    "half_priors = np.diag(priors)\n",
    "random_idx = np.random.choice(range(len(dataset[0])), int(.75 * len(dataset[0])), replace=False)\n",
    "priors = np.zeros(len(dataset[0]))\n",
    "priors[random_idx] = 1\n",
    "tq_priors = np.diag(priors)\n",
    "none_priors = np.zeros((len(dataset[0]), len(dataset[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da4518f6-8c58-4695-a919-97b5f12e8b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use random seed: 666\n",
      "Shape of Raw data\n",
      "Dataset 0: (9190, 28930)\n",
      "Dataset 1: (9190, 241757)\n",
      "---------------------------------\n",
      "Train coupled autoencoders\n",
      "Epoch: 100 - KL: 0.0130  Rec: 1.7942  CosSim: 0.0249  F: 0.0000\n",
      "Finished Mapping!\n",
      "---------------------------------\n",
      "JAMIE Done!\n",
      "Distance: 0.0038958429940976202\n",
      "Distance Memory: Stored 7680 - Peak 12284\n",
      "Correspondence: 2.2423492219968466\n",
      "Correspondence Memory: Stored 1351445975 - Peak 1351445991\n",
      "Mapping: 692.8370567480015\n",
      "Mapping Memory: Stored 642474470 - Peak 18380613287\n",
      "Total: 695.0833018129924\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jm = JAMIE(epoch_DNN=100, debug=True, enable_memory_logging=True, loss_weights=[1,1,1,0], use_f_tilde=False)\n",
    "jm_data = jm.fit_transform(dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f000b6-520d-4fab-8735-aab3ea5c221e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use random seed: 666\n",
      "Shape of Raw data\n",
      "Dataset 0: (9190, 28930)\n",
      "Dataset 1: (9190, 241757)\n",
      "Device: cpu\n",
      "---------------------------------\n",
      "Find correspondence between Dataset 1 and Dataset 2\n",
      "epoch:[500/2000] err:31392.3984 alpha:3819.6143\n",
      "epoch:[1000/2000] err:8624.8457 alpha:1050.6832\n",
      "epoch:[1500/2000] err:2395.9426 alpha:291.6512\n",
      "epoch:[2000/2000] err:198.7031 alpha:16.8099\n",
      "Finished Matching!\n",
      "---------------------------------\n",
      "Train coupled autoencoders\n",
      "Epoch: 100 - KL: 0.0032  Rec: 0.5895  CosSim: 0.0396  F: 0.0054\n",
      "Epoch: 200 - KL: 0.0059  Rec: 0.4775  CosSim: 0.0296  F: 0.0055\n",
      "Epoch: 300 - KL: 0.0069  Rec: 0.3571  CosSim: 0.0194  F: 0.0047\n",
      "Epoch: 400 - KL: 0.0098  Rec: 0.3073  CosSim: 0.0146  F: 0.0041\n",
      "Epoch: 500 - KL: 0.0141  Rec: 0.2582  CosSim: 0.0149  F: 0.0037\n",
      "epoch:[500/10000]: loss:0.305221\n",
      "Epoch: 600 - KL: 0.0224  Rec: 0.2575  CosSim: 0.0092  F: 0.0027\n",
      "Epoch: 700 - KL: 0.0287  Rec: 0.2464  CosSim: 0.0091  F: 0.0028\n",
      "Epoch: 800 - KL: 0.0362  Rec: 0.2370  CosSim: 0.0085  F: 0.0024\n",
      "Epoch: 900 - KL: 0.0588  Rec: 0.2187  CosSim: 0.0090  F: 0.0022\n",
      "Epoch: 1000 - KL: 0.0605  Rec: 0.2174  CosSim: 0.0118  F: 0.0022\n",
      "epoch:[1000/10000]: loss:0.291509\n",
      "Epoch: 1100 - KL: 0.0720  Rec: 0.2065  CosSim: 0.0159  F: 0.0023\n",
      "Epoch: 1200 - KL: 0.0990  Rec: 0.2026  CosSim: 0.0186  F: 0.0021\n",
      "Epoch: 1300 - KL: 0.1084  Rec: 0.2046  CosSim: 0.0216  F: 0.0019\n",
      "Epoch: 1400 - KL: 0.1302  Rec: 0.1922  CosSim: 0.0228  F: 0.0021\n",
      "Epoch: 1500 - KL: 0.1490  Rec: 0.1880  CosSim: 0.0274  F: 0.0021\n",
      "epoch:[1500/10000]: loss:0.368218\n",
      "Epoch: 1600 - KL: 0.1551  Rec: 0.1896  CosSim: 0.0297  F: 0.0020\n",
      "Epoch: 1700 - KL: 0.1646  Rec: 0.2073  CosSim: 0.0312  F: 0.0024\n",
      "Epoch: 1800 - KL: 0.1776  Rec: 0.1788  CosSim: 0.0313  F: 0.0020\n",
      "Epoch: 1900 - KL: 0.1776  Rec: 0.1877  CosSim: 0.0311  F: 0.0017\n",
      "Epoch: 2000 - KL: 0.1811  Rec: 0.1848  CosSim: 0.0348  F: 0.0020\n",
      "epoch:[2000/10000]: loss:0.396342\n",
      "Epoch: 2100 - KL: 0.1834  Rec: 0.1833  CosSim: 0.0326  F: 0.0017\n",
      "Epoch: 2200 - KL: 0.1838  Rec: 0.1722  CosSim: 0.0332  F: 0.0015\n",
      "Epoch: 2300 - KL: 0.1850  Rec: 0.1769  CosSim: 0.0328  F: 0.0015\n",
      "Epoch: 2400 - KL: 0.1871  Rec: 0.1611  CosSim: 0.0332  F: 0.0015\n",
      "Epoch: 2500 - KL: 0.1854  Rec: 0.1673  CosSim: 0.0332  F: 0.0016\n",
      "epoch:[2500/10000]: loss:0.392308\n",
      "Epoch: 2600 - KL: 0.1862  Rec: 0.1661  CosSim: 0.0338  F: 0.0016\n",
      "Epoch: 2700 - KL: 0.1869  Rec: 0.1662  CosSim: 0.0348  F: 0.0013\n",
      "Epoch: 2800 - KL: 0.1887  Rec: 0.1618  CosSim: 0.0338  F: 0.0014\n",
      "Epoch: 2900 - KL: 0.1922  Rec: 0.1623  CosSim: 0.0340  F: 0.0015\n",
      "Epoch: 3000 - KL: 0.1878  Rec: 0.1598  CosSim: 0.0333  F: 0.0013\n",
      "epoch:[3000/10000]: loss:0.388165\n",
      "Epoch: 3100 - KL: 0.1884  Rec: 0.1621  CosSim: 0.0343  F: 0.0013\n",
      "Epoch: 3200 - KL: 0.1902  Rec: 0.1579  CosSim: 0.0333  F: 0.0014\n",
      "Epoch: 3300 - KL: 0.1914  Rec: 0.1601  CosSim: 0.0340  F: 0.0017\n",
      "Epoch: 3400 - KL: 0.1875  Rec: 0.1684  CosSim: 0.0344  F: 0.0015\n",
      "Epoch: 3500 - KL: 0.1889  Rec: 0.1617  CosSim: 0.0342  F: 0.0013\n",
      "epoch:[3500/10000]: loss:0.379795\n",
      "Epoch: 3600 - KL: 0.1874  Rec: 0.1542  CosSim: 0.0336  F: 0.0013\n",
      "Epoch: 3700 - KL: 0.1902  Rec: 0.1545  CosSim: 0.0339  F: 0.0013\n",
      "Epoch: 3800 - KL: 0.1899  Rec: 0.1578  CosSim: 0.0331  F: 0.0011\n",
      "Epoch: 3900 - KL: 0.1896  Rec: 0.1468  CosSim: 0.0342  F: 0.0012\n",
      "Epoch: 4000 - KL: 0.1883  Rec: 0.1525  CosSim: 0.0333  F: 0.0012\n",
      "epoch:[4000/10000]: loss:0.378033\n",
      "Epoch: 4100 - KL: 0.1874  Rec: 0.1569  CosSim: 0.0330  F: 0.0010\n",
      "Epoch: 4200 - KL: 0.1866  Rec: 0.1531  CosSim: 0.0341  F: 0.0012\n",
      "Finished Mapping!\n",
      "---------------------------------\n",
      "JAMIE Done!\n",
      "Distance: 2049.9947009979987\n",
      "Distance Memory: Stored 1351651432 - Peak 31918977982\n",
      "Correspondence: 48862.850224418\n",
      "Correspondence Memory: Stored 337988448 - Peak 1351302939\n",
      "Mapping: 7087.938613008999\n",
      "Mapping Memory: Stored 643062686 - Peak 18380622080\n",
      "Total: 58000.78353842499\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# jm = JAMIE(debug=True, enable_memory_logging=True)\n",
    "# jm_data = jm.fit_transform(dataset=dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmacom",
   "language": "python",
   "name": "nmacom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
